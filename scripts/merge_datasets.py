#!/usr/bin/env python3
"""
åˆå¹¶æ•°æ®é›†è„šæœ¬ - ç”¨äºåˆå¹¶é€šè¿‡retarget_motion.pyç”Ÿæˆçš„æ•°æ®é›†

æ”¯æŒçš„æ•°æ®æ ¼å¼ï¼š
1. å•ä¸ªè¿åŠ¨æ–‡ä»¶ï¼šmotion_im.p
2. å¤šä¸ªè¿åŠ¨æ–‡ä»¶ï¼š{dataset_name}/{dataset_name}.pkl

ä½¿ç”¨æ–¹æ³•ï¼š
python scripts/merge_datasets.py dataset1_path dataset2_path output_path [--prefix1 name1] [--prefix2 name2]
"""

import os
import argparse
import joblib
import numpy as np
from collections import defaultdict


def load_dataset(dataset_path):
    """
    åŠ è½½æ•°æ®é›†æ–‡ä»¶
    
    Args:
        dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: åŠ è½½çš„æ•°æ®é›†å­—å…¸
    """
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
    
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {dataset_path}")
    data = joblib.load(dataset_path)
    
    # æ£€æŸ¥æ•°æ®æ ¼å¼
    if isinstance(data, dict):
        # æ£€æŸ¥æ˜¯å¦æ˜¯å•ä¸ªè¿åŠ¨æ•°æ®ï¼ˆåŒ…å«root_trans_offsetç­‰ç›´æ¥å­—æ®µï¼‰
        if "root_trans_offset" in data:
            # å•ä¸ªè¿åŠ¨æ•°æ®ï¼ŒåŒ…è£…æˆå­—å…¸æ ¼å¼
            filename = os.path.splitext(os.path.basename(dataset_path))[0]
            data = {filename: data}
            print(f"  âœ“ æ£€æµ‹åˆ°å•ä¸ªè¿åŠ¨æ–‡ä»¶ï¼ŒåŒ…è£…ä¸º: {filename}")
        else:
            print(f"  âœ“ æ£€æµ‹åˆ°å¤šè¿åŠ¨æ•°æ®é›†ï¼ŒåŒ…å« {len(data)} ä¸ªè¿åŠ¨")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(data)}")
    
    return data


def validate_motion_data(motion_key, motion_data):
    """
    éªŒè¯å•ä¸ªè¿åŠ¨æ•°æ®çš„å®Œæ•´æ€§
    
    Args:
        motion_key: è¿åŠ¨æ•°æ®çš„é”®å
        motion_data: è¿åŠ¨æ•°æ®å­—å…¸
        
    Returns:
        bool: éªŒè¯æ˜¯å¦é€šè¿‡
    """
    required_fields = [
        "root_trans_offset", "dof", "dof_increment", 
        "default_joint_angles", "root_rot", "smpl_joints", "fps"
    ]
    
    missing_fields = []
    for field in required_fields:
        if field not in motion_data:
            missing_fields.append(field)
    
    if missing_fields:
        print(f"  âš ï¸  è¿åŠ¨ {motion_key} ç¼ºå°‘å­—æ®µ: {missing_fields}")
        return False
    
    # æ£€æŸ¥æ•°ç»„é•¿åº¦ä¸€è‡´æ€§
    try:
        n_frames = len(motion_data["root_trans_offset"])
        arrays_to_check = ["dof", "dof_increment", "root_rot", "smpl_joints"]
        
        for array_name in arrays_to_check:
            if array_name in motion_data:
                if len(motion_data[array_name]) != n_frames:
                    print(f"  âš ï¸  è¿åŠ¨ {motion_key} çš„ {array_name} é•¿åº¦ä¸åŒ¹é…: {len(motion_data[array_name])} vs {n_frames}")
                    return False
        
        # æ£€æŸ¥task_info
        if "task_info" in motion_data:
            task_info = motion_data["task_info"]
            if "target_vel" in task_info and len(task_info["target_vel"]) != n_frames:
                print(f"  âš ï¸  è¿åŠ¨ {motion_key} çš„ task_info.target_vel é•¿åº¦ä¸åŒ¹é…")
                return False
        
        print(f"  âœ“ è¿åŠ¨ {motion_key}: {n_frames} å¸§ï¼Œæ•°æ®å®Œæ•´")
        return True
        
    except Exception as e:
        print(f"  âŒ è¿åŠ¨ {motion_key} éªŒè¯å¤±è´¥: {e}")
        return False


def check_compatibility(dataset1, dataset2):
    """
    æ£€æŸ¥ä¸¤ä¸ªæ•°æ®é›†çš„å…¼å®¹æ€§
    
    Args:
        dataset1: ç¬¬ä¸€ä¸ªæ•°æ®é›†
        dataset2: ç¬¬äºŒä¸ªæ•°æ®é›†
        
    Returns:
        bool: æ˜¯å¦å…¼å®¹
    """
    print("ğŸ” æ£€æŸ¥æ•°æ®é›†å…¼å®¹æ€§...")
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªè¿åŠ¨æ•°æ®æ£€æŸ¥æ ¼å¼
    sample1 = next(iter(dataset1.values()))
    sample2 = next(iter(dataset2.values()))
    
    # æ£€æŸ¥FPSæ˜¯å¦ä¸€è‡´
    fps1 = sample1.get("fps", 30)
    fps2 = sample2.get("fps", 30)
    if fps1 != fps2:
        print(f"  âš ï¸  FPSä¸ä¸€è‡´: æ•°æ®é›†1={fps1}, æ•°æ®é›†2={fps2}")
    
    # æ£€æŸ¥å…³èŠ‚æ•°é‡æ˜¯å¦ä¸€è‡´
    dof1_shape = sample1["dof"].shape[-1] if "dof" in sample1 else 0
    dof2_shape = sample2["dof"].shape[-1] if "dof" in sample2 else 0
    if dof1_shape != dof2_shape:
        print(f"  âŒ å…³èŠ‚æ•°é‡ä¸åŒ¹é…: æ•°æ®é›†1={dof1_shape}, æ•°æ®é›†2={dof2_shape}")
        return False
    
    # æ£€æŸ¥é»˜è®¤å…³èŠ‚è§’åº¦æ˜¯å¦ä¸€è‡´
    default1 = sample1.get("default_joint_angles")
    default2 = sample2.get("default_joint_angles")
    if default1 is not None and default2 is not None:
        if not np.allclose(default1, default2, atol=1e-6):
            print(f"  âš ï¸  é»˜è®¤å…³èŠ‚è§’åº¦ä¸å®Œå…¨ä¸€è‡´ï¼Œå¯èƒ½æ¥è‡ªä¸åŒæœºå™¨äººé…ç½®")
    
    print(f"  âœ“ æ•°æ®é›†å…¼å®¹ï¼Œå…³èŠ‚æ•°é‡: {dof1_shape}")
    return True


def resolve_key_conflicts(dataset1, dataset2, prefix1="", prefix2=""):
    """
    è§£å†³é”®åå†²çª
    
    Args:
        dataset1: ç¬¬ä¸€ä¸ªæ•°æ®é›†
        dataset2: ç¬¬äºŒä¸ªæ•°æ®é›†
        prefix1: ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„å‰ç¼€
        prefix2: ç¬¬äºŒä¸ªæ•°æ®é›†çš„å‰ç¼€
        
    Returns:
        tuple: å¤„ç†åçš„ä¸¤ä¸ªæ•°æ®é›†
    """
    keys1 = set(dataset1.keys())
    keys2 = set(dataset2.keys())
    conflicts = keys1 & keys2
    
    if conflicts:
        print(f"ğŸ”€ æ£€æµ‹åˆ° {len(conflicts)} ä¸ªé”®åå†²çª: {list(conflicts)[:5]}...")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå‰ç¼€ï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if not prefix1:
            prefix1 = "dataset1_"
        if not prefix2:
            prefix2 = "dataset2_"
        
        print(f"  ä½¿ç”¨å‰ç¼€è§£å†³å†²çª: '{prefix1}' å’Œ '{prefix2}'")
        
        # é‡å‘½åå†²çªçš„é”®
        new_dataset1 = {}
        new_dataset2 = {}
        
        for key, value in dataset1.items():
            if key in conflicts:
                new_key = prefix1 + key
                new_dataset1[new_key] = value
                print(f"    {key} â†’ {new_key}")
            else:
                new_dataset1[key] = value
        
        for key, value in dataset2.items():
            if key in conflicts:
                new_key = prefix2 + key
                new_dataset2[new_key] = value
                print(f"    {key} â†’ {new_key}")
            else:
                new_dataset2[key] = value
        
        return new_dataset1, new_dataset2
    else:
        print("âœ“ æ— é”®åå†²çª")
        return dataset1, dataset2


def merge_datasets(dataset1, dataset2, prefix1="", prefix2=""):
    """
    åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†
    
    Args:
        dataset1: ç¬¬ä¸€ä¸ªæ•°æ®é›†
        dataset2: ç¬¬äºŒä¸ªæ•°æ®é›†
        prefix1: ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„å‰ç¼€
        prefix2: ç¬¬äºŒä¸ªæ•°æ®é›†çš„å‰ç¼€
        
    Returns:
        dict: åˆå¹¶åçš„æ•°æ®é›†
    """
    print("ğŸ”€ å¼€å§‹åˆå¹¶æ•°æ®é›†...")
    
    # éªŒè¯æ•°æ®å®Œæ•´æ€§
    print("ğŸ“‹ éªŒè¯æ•°æ®é›†1...")
    valid_count1 = 0
    for key, motion_data in dataset1.items():
        if validate_motion_data(key, motion_data):
            valid_count1 += 1
    
    print("ğŸ“‹ éªŒè¯æ•°æ®é›†2...")
    valid_count2 = 0
    for key, motion_data in dataset2.items():
        if validate_motion_data(key, motion_data):
            valid_count2 += 1
    
    print(f"æ•°æ®é›†1: {valid_count1}/{len(dataset1)} ä¸ªæœ‰æ•ˆè¿åŠ¨")
    print(f"æ•°æ®é›†2: {valid_count2}/{len(dataset2)} ä¸ªæœ‰æ•ˆè¿åŠ¨")
    
    # æ£€æŸ¥å…¼å®¹æ€§
    if not check_compatibility(dataset1, dataset2):
        raise ValueError("æ•°æ®é›†ä¸å…¼å®¹ï¼Œæ— æ³•åˆå¹¶")
    
    # è§£å†³é”®åå†²çª
    dataset1, dataset2 = resolve_key_conflicts(dataset1, dataset2, prefix1, prefix2)
    
    # åˆå¹¶æ•°æ®
    merged_dataset = {}
    merged_dataset.update(dataset1)
    merged_dataset.update(dataset2)
    
    print(f"âœ… åˆå¹¶å®Œæˆï¼æ€»å…± {len(merged_dataset)} ä¸ªè¿åŠ¨")
    return merged_dataset


def save_merged_dataset(merged_dataset, output_path):
    """
    ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†
    
    Args:
        merged_dataset: åˆå¹¶åçš„æ•°æ®é›†
        output_path: è¾“å‡ºè·¯å¾„
    """
    print(f"ğŸ’¾ ä¿å­˜åˆå¹¶æ•°æ®é›†åˆ°: {output_path}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # ä¿å­˜æ•°æ®
    joblib.dump(merged_dataset, output_path)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_frames = 0
    total_duration = 0
    fps_list = []
    
    for motion_data in merged_dataset.values():
        frames = len(motion_data["root_trans_offset"])
        fps = motion_data.get("fps", 30)
        total_frames += frames
        total_duration += frames / fps
        fps_list.append(fps)
    
    avg_fps = np.mean(fps_list) if fps_list else 30
    
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  è¿åŠ¨æ•°é‡: {len(merged_dataset)}")
    print(f"  æ€»å¸§æ•°: {total_frames}")
    print(f"  æ€»æ—¶é•¿: {total_duration:.1f} ç§’")
    print(f"  å¹³å‡FPS: {avg_fps:.1f}")
    print(f"âœ… ä¿å­˜å®Œæˆ!")


def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶retarget motionç”Ÿæˆçš„æ•°æ®é›†")
    parser.add_argument("dataset1", help="ç¬¬ä¸€ä¸ªæ•°æ®é›†è·¯å¾„")
    parser.add_argument("dataset2", help="ç¬¬äºŒä¸ªæ•°æ®é›†è·¯å¾„") 
    parser.add_argument("output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--prefix1", default="", help="ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„å‰ç¼€ï¼ˆè§£å†³é”®åå†²çªï¼‰")
    parser.add_argument("--prefix2", default="", help="ç¬¬äºŒä¸ªæ•°æ®é›†çš„å‰ç¼€ï¼ˆè§£å†³é”®åå†²çªï¼‰")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(args.output) and not args.force:
        response = input(f"è¾“å‡ºæ–‡ä»¶ {args.output} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–ï¼Ÿ(y/N): ")
        if response.lower() != 'y':
            print("æ“ä½œå–æ¶ˆ")
            return
    
    try:
        # åŠ è½½æ•°æ®é›†
        dataset1 = load_dataset(args.dataset1)
        dataset2 = load_dataset(args.dataset2)
        
        # åˆå¹¶æ•°æ®é›†
        merged_dataset = merge_datasets(dataset1, dataset2, args.prefix1, args.prefix2)
        
        # ä¿å­˜ç»“æœ
        save_merged_dataset(merged_dataset, args.output)
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main()) 